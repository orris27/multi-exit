{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Techniques for Training Adaptive Deep Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Work\n",
    "#### Adaptive Inference\n",
    "save computation on \"easy\" samples\n",
    "\n",
    "Approach:\n",
    "learn adaptive network topology selection policies\n",
    "\n",
    "#### Knowledge Distillation\n",
    "Outputs from teach network are utilized to supervise the training of the student network\n",
    "\n",
    "### Contributions\n",
    "+ gradient equilibrium: resolve the gradients conflict of different classifiers\n",
    "+ inline subnetwork collaboration & one-for-all knowledge distillation: enhance the collaboration among classifiers\n",
    "\n",
    "\n",
    "#### 1. gradient equilibrium\n",
    "Weighted cumulative loss function may lead to a gradient imblance issue due to the overlap of the network. The gradients of overlapping networks will become very large, because their gradients come from all exiting points behind them.\n",
    "\n",
    "\n",
    "Suppose we have $k$ classifiers. For $i$th branch (indexing from 1), we rescale the gradients from $i$th classifier by $\\frac{1}{k - i + 1}$ and that from the subsequent $(k - i)$ classifiers by $\\frac{k - i}{k - i + 1}$.\n",
    "\n",
    "For example, if we have $3$ blocks and each block has $1$ classifier. Then, for each classifier, its parameters receive full gradients from backward, while for $i$th block, its parameters obtain $\\frac{1}{k-i+1}$ gradients from $i$th classifier and extra gradients from the subsequent classifiers.\n",
    "\n",
    "<img src=\"images/gradient_equilibrium.png\"/>\n",
    "\n",
    "#### 2. Collaboration\n",
    "In previous works, they treat multiple classifiers independently, expecting that their losses are simply summed up during training process.\n",
    "+ Inline Subnetwork Collaboration(ISC): Add a connection from $i$-th classifier to $i+1$-th classifier in forward process, but ignore the gradients in backward process (to prevent the early classiifers being influenced by the latter ones)\n",
    "+ One_for_all Knowledge Distillation(OFA): We use logits of $i$-th classifier as the knowledge to facilitate the learning of its subsequent classifier\n",
    "+ The loss function of $i$-th classifier consists of cross-entropy loss and the alignment of soft class probabilities between the teacher and student models using the Kullback Leibler divergence.\n",
    "\n",
    "ISC:\n",
    "We continue on the illustration above, and the only difference is to add the connection between the logits at previous layer and current classifier. The previous logits are concatenated with current logits and then transformed by a a simple network (e.g. fully-connected layer), which is fed into the current classifier.\n",
    "![](images/inline_subnetwork_collaboration.png)\n",
    "\n",
    "OFA:\n",
    "The final classifier is trained using cross entropy with labels, while the early classifiers are trained using the combination of cross entropy with labels and knowledge of logits from the final classifier.\n",
    "![](images/one_for_all_knowledge_distillation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
