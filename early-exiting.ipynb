{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Fast Inference with Early Exit](https://www.intel.ai/fast-inference-with-early-exit/#gs.oc5n5e)\n",
    "\n",
    " a confidence measure will determine if a prediction made at a certain stage can exit early from the entire deep learning topology, thus saving unnecessary processing in the subsequent layers.\n",
    " \n",
    "+ Leverage the variance of difficulty among real-world data and thus uses only part of the network to handle recognition tasks: Conditional Deep Learning for Energy-Efficient and Enhanced Pattern Recognition\n",
    "+ Multi-Scale Dense Network (MSDNet) \n",
    "+ selectively inserting exits between specific layers. Checks for an ability to exit were done after some amount of extra processing on the exit branches themselves: BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks.\n",
    "+ perform dynamic routing of the data and thus skip certain layers of processing along the way: SkipNet: Learning Dynamic Routing in Convolutional Networks\n",
    "\n",
    "\n",
    "[Distiller](https://ai.intel.com/compressing-deep-learning-models-with-neural-network-distiller/): nn compression research\n",
    "\n",
    "\n",
    "[Early exit](https://nervanasystems.github.io/distiller/algo_earlyexit/index.html) is a new feature in Distiller and is available as an Open Source package on [Github](https://github.com/NervanaSystems/distiller).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### BranchyNet\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/677674e81070879f7b6da6261d0ba174985a3cf6/1-Figure1-1.png\" width=\"150\"/>\n",
    "additional side branch classifiers\n",
    "\n",
    "+ For many simple test examples can exit the network early via these branches when they are infered with high confidence.\n",
    "+ For difficult examples, deeper networks are still utilized.\n",
    "\n",
    "\n",
    "Jointly Optimization: \n",
    "+ optimizes the weighted loss of all exit points\n",
    "+ each exit point provides regularization on others?\n",
    "+ The final loss function (N is the total number of exiting point): \n",
    "\n",
    "$$\n",
    "L_{branchynet}(\\hat{\\mathbf{y}}, \\mathbf{y}; \\theta) = \\sum_{n=1}^N { w_n L(\\hat{\\mathbf{y}}_{exit_n}, \\mathbf{y}; \\theta)  }\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Structures of Branches:\n",
    "+ The computation cost of branch should be less than that of exiting at a later exit point\n",
    "+ Earlier branch has more layers, and later branch has fewer layers.\n",
    "\n",
    "#### Q&A\n",
    "\n",
    "How do we choose weight $w_n$ for each exit point?\n",
    "+ higher weights at early exiting point\n",
    "\n",
    "How do we messure the confidence of an exiting point?\n",
    "+ Compute the entropy value at the exiting point: $\\text{entropy}(\\mathbf{y}) = \\sum_{c\\in C}{ y_c \\log {y_c} }$, where $C$ is the set of classes.\n",
    "+ $y_c$ is obtained by forward pass on the sub-networks\n",
    "\n",
    "How do we determine the exiting threshold $T_n$ for exiting point?\n",
    "+ determined by application, i.e., as long as the accuracy and speed meets the requirements.\n",
    "\n",
    "How do we choose the exiting location?\n",
    "+ Depends on the difficulty of the task (dataset, etc.)\n",
    "\n",
    "Do we forward from the first layer again for the future exiting points? I think there is more computation loss.\n",
    "\n",
    "\n",
    "\n",
    "### SkipNet\n",
    "![](https://d3i71xaburhd42.cloudfront.net/f37ea0b173dd0403a5028c12746082d31dff60bb/4-Figure2-1.png)\n",
    "Modify ResNet\n",
    "\n",
    "Gating Network:\n",
    "+ Add to each residual network: takes the outputs of the previous layer as inputs, and outputs 0/1 to determine whether to skip the block (1: no-skip; 0: skip)\n",
    "+ non-differentiable\n",
    "    - ~~gradient descent~~\n",
    "    - softmax softer? make it differentiable\n",
    "    - fidelity + penalty (=reward)\n",
    "    \n",
    "Loss Function:\n",
    "$$\n",
    "L_{\\theta}(g, X) = L(\\hat{y}(X, F_{\\theta}, g), y) - \\frac{\\alpha}{N} \\sum_{i=1}^N{(1 - g_i ) C_i},\n",
    "$$\n",
    "where $g_i$ is the $i$th decision (0/1), $N$ is the number of decisions, $C_i$ is hyperparamter to measure the importance of $F_{\\theta}^i$ (the authors select $C_i$ as 1), $\\alpha$ is another hyperparameter, $F_{\\theta}^i$ is the set of network parameters for $i$th layer including gating network, $F_{\\theta} = [F_{\\theta}^1, F_{\\theta}^2, \\cdots, F_{\\theta}^N]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSDNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlockDrop\n",
    "dynamically remove networks while keeping high accuracy\n",
    "\n",
    "\n",
    "\n",
    "Assumptions:\n",
    "+ different blocks do not share strong dependencies, but we cannot remove too many layers\n",
    "\n",
    "\n",
    "the residual blocks that are kept for evaluation can be further pruned to speed up\n",
    "\n",
    "instance-specific residual block removal scheme\n",
    "\n",
    "Dropping layers => regularization: Dropout, DropConnect\n",
    "+ dropping only happens in training, not inference\n",
    "\n",
    "\n",
    "*Formula 1.* We bound probability using the following formula:\n",
    "$$\n",
    "s^\\prime = \\alpha s + (1 - \\alpha) (1 - s),\n",
    "$$\n",
    "where $s_i$ is the probablity of preserving $i$th block, $\\alpha$ is the hyperparameter selected as $0.8$ in authors' codes. In codes, the shape of $s$ is $(\\text{batch_size}, \\text{num_blocks})$. We use $s$ to initialize Bernoulli parameters and stochastically sample 0/1.\n",
    "```python\n",
    "import torch\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "s = torch.Tensor([[0.23, 0.46, 0.75, 0.52], [0.35, 0.29, 0.52, 0.58]])\n",
    "distr = Bernoulli(s)\n",
    "distr.sample() # tensor([[0., 1., 0., 0.], [0., 0., 1., 0.]])\n",
    "```\n",
    "\n",
    "*Formula 2.* Advantage is computed as subtraction of reward based on Bernoulli distribution from that based on maximally probable configuration.\n",
    "$$\n",
    "A = R(u) - R(\\tilde{u}),\n",
    "$$\n",
    "where $u$ is computed with the above `Bernoulli(s).sample()`, but $\\tilde{u}$ is calculated with `s[s>=0.5] = 1.0; s[s<0.5] = 0.0;`\n",
    "\n",
    "*Formula 3.* The loss function is defined as follows (NOTE: the authors subtract it from the entropy loss and then back-propagate):\n",
    "$$\n",
    "\\bigtriangledown_W J = \\mathbb{E}[A \\bigtriangledown_W \\sum_{k=1}^K \\log[s_k u_k + (1 - s_k) (1 - u_k)]], \n",
    "$$\n",
    "where $s_k u_k + (1 - s_k) (1 - u_k)$ is the probability density value of Bernoulli at value $u_k$. For example, if $s_k = 0.46, u_k = 1$, then it equals to $0.46$. If $s_k = 0.46, u_k = 0$, then it equals to $0.54$. Therefore, the loss function above multiplies the probability of policy with its corresponding advantage.\n",
    "\n",
    "*Formula 4.* High reward is given if we achieve low block utilization and correct prediction:\n",
    "$$\n",
    "R(u) = \\begin{cases}\n",
    "1-\\left(\\frac{|u|_0}{K}\\right)^2 & \\text{if correct}\\\\\n",
    "-\\gamma, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\gamma$ is a penalty hyperparameter which is selected as $1$ in official codes. $|u|_0$ is the number of blocks that are preserved, and $K$ is the total number of blocks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
