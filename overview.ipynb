{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "**HMDB-51**\n",
    "\n",
    "**UCF-101.** [Official Website](https://www.crcv.ucf.edu/data/UCF101.php). UCF-101 contains 101 human action classes including haircut, playing guitar, billiard, fencing and etc.\n",
    "\n",
    "Categories can be divided into five types:\n",
    "1. Human-Object Interation\n",
    "2. Body-Motion Only\n",
    "3. Human-Human Interation\n",
    "4. Playing Musical Instruments\n",
    "5. Sports\n",
    "\n",
    "List of action classes and their numerical index: [Download](https://www.crcv.ucf.edu/THUMOS14/Class%20Index.txt)\n",
    "\n",
    "All clips in UCF-101 are from only 2.5k distinct videos. The problem with it is that, for example, the class of \"brushing hair\" contains the 7 clips from one video of one person.\n",
    "\n",
    "\n",
    "**Kinetics.** Kinetics has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video.\n",
    "+ considerable camera motion/shake, illuminstration variations, shadows, background clutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "+ ConvNets with an LSTM on top: \n",
    "    - Long-term recurrent convolutional networks for visual recognition and description\n",
    "    - Beyond Short Snippets: Deep Networks for Video Classification\n",
    "+ two-stream networks\n",
    "    - Convolutional Two-Stream Network Fusion for Video Action Recognition\n",
    "    - Two-stream convolutional networks for action recognition in videos.\n",
    "+ 3D ConvNet\n",
    "    - Convolutional learning of spatio-temporal features 2010\n",
    "    - 3d convolutional neural networks for human action recognition 2012\n",
    "    - Learning spatiotemporal features with 3d convolutional networks (C3D) 2014\n",
    "\n",
    "<img src=\"images/video_architecture.png\" width=\"450\"/>\n",
    "\n",
    "#### Two-Stream networks\n",
    "Pass a single RGB frame and a stack of 10 externally computed optical flow frames through two replicas of an ImageNet-pretrained ConvNet and average their predictions.\n",
    "\n",
    "#### 2D ConvNet\n",
    "+ only consider spatial information, do not capture the motion information encoded in multiple contiguous frames for video analysis problems\n",
    "+ ConvNets with LSTMs on top & two-stream networks with 2 different types of stream fusion\n",
    "+ We use 2D CNN before. (Remember we use `torch.nn.Conv2d`?)\n",
    "\n",
    "#### C3D\n",
    "[Learning spatiotemporal features with 3d convolutional networks](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf).\n",
    "\n",
    "\n",
    "+ 8 convolutional layer\n",
    "+ 5 pooling layer\n",
    "+ 2 fully connected layer\n",
    "+ inputs: 16-frame clips with 112x112-pixel crops\n",
    "+ In [The Kinetics Human Action Video Dataset](https://arxiv.org/pdf/1705.06950.pdf), the authors add batch normalization after all convolutional and fully connected layers, and use a temporal stride of 2 instead of 1 in the first pooling layer to reduce the memory footprint and allows for bigger batches (important for batch normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are the same\n"
     ]
    }
   ],
   "source": [
    "# 2D Convolution\n",
    "import torch\n",
    "conv = torch.nn.Conv2d(3, 4, 3) # C_O, C_I, K\n",
    "conv.weight\n",
    "x = torch.randn(1, 3, 6, 8) # B, C, H, W\n",
    "y_torch = conv(x)\n",
    "\n",
    "def my_conv2d(x, conv):\n",
    "    y = torch.zeros(x.shape[0], conv.weight.data.shape[0], x.shape[2] - conv.weight.data.shape[2] + 1, x.shape[3] - conv.weight.data.shape[3] + 1)\n",
    "    for b in range(y.shape[0]): # batch size\n",
    "        for o in range(y.shape[1]): # output channel\n",
    "            for h in range(y.shape[2]): # output height\n",
    "                for w in range(y.shape[3]): # output width\n",
    "                    y[b, o, h, w] = torch.sum(x[b, :, h:h+3, w:w+3] * conv.weight.data[o, :, :, :]) + conv.bias[o]\n",
    "    return y\n",
    "\n",
    "y_mine = my_conv2d(x, conv)\n",
    "                \n",
    "print('Results are the same.' if torch.any(torch.isclose(y_torch, y_mine, 1e-5)).item() == 1 else 'Results are different.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are the same.\n"
     ]
    }
   ],
   "source": [
    "# 3D Convolution\n",
    "import torch\n",
    "conv = torch.nn.Conv3d(3, 4, 3) # C_O, C_I, K\n",
    "conv.weight\n",
    "x = torch.randn(1, 3, 10, 6, 8) # B, C, D(Depth), H, W\n",
    "y_torch = conv(x) # B, C_O, D, H, W\n",
    "\n",
    "def my_conv3d(x, conv):\n",
    "    y = torch.zeros(x.shape[0], conv.weight.data.shape[0], x.shape[2] - conv.weight.data.shape[2] + 1, x.shape[3] - conv.weight.data.shape[3] + 1, x.shape[4] - conv.weight.data.shape[4] + 1)\n",
    "    for b in range(y.shape[0]): # batch size\n",
    "        for o in range(y.shape[1]): # output channel\n",
    "            for d in range(y.shape[2]): # output depth\n",
    "                for h in range(y.shape[3]): # output height\n",
    "                    for w in range(y.shape[4]): # output width\n",
    "                        y[b, o, d, h, w] = torch.sum(x[b, :, d:d+3, h:h+3, w:w+3] * conv.weight.data[o, :, :, :, :]) + conv.bias[o]\n",
    "    return y\n",
    "\n",
    "y_mine = my_conv3d(x, conv)\n",
    "\n",
    "\n",
    "print('Results are the same.' if torch.any(torch.isclose(y_torch, y_mine, 1e-5)).item() == 1 else 'Results are different.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video as Sequences\n",
    "Design choices:\n",
    "+ RGB\n",
    "+ optical flow\n",
    "+ RGB + optical flow\n",
    "\n",
    "**3D Convolutional Neural Networks for Human Action Recognition.** Multiple channels as inputs: \n",
    "1. gray\n",
    "2. gradient x\n",
    "3. gradient y\n",
    "4. optical flow x\n",
    "5. optical flow y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
